{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn import dummy #Majority classifier\n",
    "from sklearn.linear_model import LogisticRegression #will be used to build custom classifier\n",
    "from sklearn.naive_bayes import GaussianNB #will be used to build custom classifier\n",
    "from sklearn.ensemble import RandomForestClassifier #will be used to build custom classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import operator\n",
    "\n",
    "\n",
    "from sklearn import cross_validation  #No train-test set splitting will be done, I'll use the library\n",
    "from sklearn import datasets\n",
    "\n",
    "input_file = \"/tmp/train.csv\"\n",
    "train = pd.read_csv(input_file, header = 0)\n",
    "input_file = \"/tmp/test.csv\"\n",
    "test = pd.read_csv(input_file, header = 0)\n",
    "\n",
    "df=pd.concat([train,test])\n",
    "\n",
    "cols_to_convert = ['job','marital','education','default','housing','loan','contact','month','day_of_week','campaign','pdays','previous','poutcome']\n",
    "le = preprocessing.LabelEncoder()\n",
    "for col in cols_to_convert:\n",
    "    df[col]=le.fit_transform(df[col])\n",
    "    \n",
    "test = df.tail(1120)\n",
    "train = df.head(df.shape[0]-1120)\n",
    "\n",
    "test = test.ix[:,0:-1]  #ycol added\n",
    "\n",
    "\n",
    "train_X=train.ix[:,0:-1]\n",
    "train_Y=train.ix[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We turn the data in a (samples, feature) matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train = Imputer().fit_transform(train)\n",
    "n_samples = len(train_Y)\n",
    "#data = train_X.reshape((n_samples, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the majority class classifier, I use sklearn.dummy.DummyClassifier.\n",
    "### Custom classifier buidling will be done later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = dummy.DummyClassifier(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89 (+/- 0.00) [Majority class classifier]\n"
     ]
    }
   ],
   "source": [
    "for key, label in zip([classifier], ['Majority class classifier']):\n",
    "    scores = cross_validation.cross_val_score(key, train_X, train_Y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very poor accuracy. It gets it right 10% of the time. No big deal, since it's a digit from [0,9]. They all have 0.1 probability of appearance. This is also a balanced dataset. No sampling bias is existent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected = digits.target[n_samples / 2:]\n",
    "predicted = classifier.predict(data[n_samples / 2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ensemble classifier building\n",
    "#### weights of the bagged classifiers are equal, it would be a good idea to tune them otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        for key in self.keys:\n",
    "            key.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        self.classes_ = np.asarray([key.predict(X) for key in self.keys])\n",
    "        maj = np.asarray([np.argmax(np.bincount(self.classes_[:,c])) for c in range(self.classes_.shape[1])])\n",
    "\n",
    "        return maj\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        self.probas_ = [key.predict_proba(X) for key in self.keys]\n",
    "        avg = np.average(self.probas_)\n",
    "\n",
    "        return avg'''\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = RandomForestClassifier(n_estimators=1500)\n",
    "estimators.append(('rf', model2))\n",
    "model3 = SVC()\n",
    "#estimators.append(('svm', model3))\n",
    "model4 = GradientBoostingClassifier(n_estimators=1500)\n",
    "#estimators.append(('XGBoost', model4))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "#results = cross_validation.cross_val_score(ensemble, X, Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91 (+/- 0.01) [Logistic Regression]\n",
      "Accuracy: 0.91 (+/- 0.01) [Random Forest]\n",
      "Accuracy: 0.90 (+/- 0.02) [XGBoost]\n",
      "Accuracy: 0.89 (+/- 0.00) [SVM]\n",
      "Accuracy: 0.91 (+/- 0.01) [ensemble]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1327)\n",
    "key1 = LogisticRegression()\n",
    "key2 = RandomForestClassifier(n_estimators=1500)\n",
    "key3 = GradientBoostingClassifier(n_estimators=1500)\n",
    "key0 = SVC()\n",
    "\n",
    "for key, label in zip([key1, key2, key3, key0, ensemble], ['Logistic Regression', 'Random Forest', 'XGBoost', 'SVM' ,'ensemble']):\n",
    "    scores = cross_validation.cross_val_score(key, train_X, train_Y, cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False)),\n",
       " ('rf',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=1500, n_jobs=1, oob_score=False,\n",
       "              random_state=None, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.fit(train_X,train_Y)\n",
    "ensemble.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = ensemble.predict(test)\n",
    "prediction.tofile(\"/tmp/final2.csv\",sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
