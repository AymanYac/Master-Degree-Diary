<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>YACHAOUI_sparkMLLIB_LAB - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html","displayName":"Databricks Guide","icon":"question"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html","displayName":"Application Examples","icon":"code"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/courses/index.html","displayName":"Training","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","maxCustomTags":45,"enableInstanceProfilesUIInJobs":false,"nodeInfo":{"node_types":[{"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"memory_mb":6144,"category":"Community Edition","num_cores":0.88,"support_ebs_volumes":false}],"default_node_type_id":"dev-tier-node"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":false,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"clusters":true,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-603ee96315e2763ac658f02088fa637d69d69ac26d76ce791f1f65cd91bd53cd","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-35f30dcf90b59210c3f253a98dbb4d255fa4a1773385163b3cec143c1271e065","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-f40841fbebd0e733dfd15b2913bc495542eacb05ba2efa43cd192aed8b307046","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-876320751f9b6e4318a1235c9d5f862196d02008ff478f483e2820c48c59d1fe","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":false,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":8,"memory-optimized":1,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":false,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.31","accountsLimit":3,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"disableLegacyDashboards":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableClusterAclsByTier":false,"disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"useFixedStaticNotebookVersionForDevelopment":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableWorkspaceAclService":true,"docsDomain":"https://docs.cloud.databricks.com/","enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"d021cfb95994095f8eba80e12423a5f1461121b6","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/A%20Gentle%20Introduction%20to%20Apache%20Spark%20on%20Databricks.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/Quick%20Start%20DataFrames.html","displayName":"Quick Start DataFrames","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/GSW%20Passing%20Analysis%20(new).html","displayName":"GSW Passing Analysis (new)","icon":"img/home/Python_icon.svg"}],"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"enableTerminal":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":true,"useFramedStaticNotebooks":true,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":1469190093917788,"name":"YACHAOUI_sparkMLLIB_LAB","language":"scala","commands":[{"version":"CommandV1","origId":1469190093917790,"guid":"bec750d0-1263-4c31-81bc-f2aa843aec45","subtype":"command","commandType":"auto","position":1.0,"command":"import org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.mllib.util.MLUtils\n\nimport sys.process._\n\n\"wget -P /tmp https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\" !!","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.mllib.util.MLUtils\nimport sys.process._\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">Unclosed block</div>","error":null,"workflows":[],"startTime":1.478642692011E12,"submitTime":1.478642692002E12,"finishTime":1.478642692399E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a635465a-a028-41f6-bb36-d66015aa20cc"},{"version":"CommandV1","origId":1469190093917792,"guid":"b74d9514-2bd1-4353-b8a2-5071c4f2ebf2","subtype":"command","commandType":"auto","position":2.0,"command":"val localpath=\"file:/tmp/sample_libsvm_data.txt.2\"\ndbutils.fs.mkdirs(\"dbfs:/datasets/\")\ndbutils.fs.cp(localpath, \"dbfs:/datasets/sample_libsvm_data.txt\")\n\nval data = MLUtils.loadLibSVMFile(sc, \"dbfs:/datasets/sample_libsvm_data.txt\")\nval splits =data.randomSplit(Array(0.7,0.3))\nval (trainingData,testData) = (splits(0),splits(1))\nval numClasses = 2\nval categoricalFeaturesInfo = Map[Int,Int]()\nval impurity=\"gini\"\nval maxDepth = 5\nval maxBins=32\n\nval featureSubsetStrategy = \"auto\" \nval numTrees = 50","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">localpath: String = file:/tmp/sample_libsvm_data.txt.2\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[462] at map at MLUtils.scala:84\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[463] at randomSplit at &lt;console&gt;:101, MapPartitionsRDD[464] at randomSplit at &lt;console&gt;:101)\ntrainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[463] at randomSplit at &lt;console&gt;:101\ntestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[464] at randomSplit at &lt;console&gt;:101\nnumClasses: Int = 2\ncategoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\nimpurity: String = gini\nmaxDepth: Int = 5\nmaxBins: Int = 32\nfeatureSubsetStrategy: String = auto\nnumTrees: Int = 50\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.io.FileNotFoundException: File file:/tmp/tmp/sample_libsvm_data.txt.2 does not exist","error":"<div class=\"ansiout\">\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.cp(DBUtilsCore.scala:67)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.cp(DbfsUtilsImpl.scala:41)</div>","workflows":[],"startTime":1.4786427968E12,"submitTime":1.478642796795E12,"finishTime":1.478642797822E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cb3e4fb7-a1ce-4d4e-8c33-fbe3eff70371"},{"version":"CommandV1","origId":1469190093917794,"guid":"d879e449-9903-4300-aa99-10e0a3dc0ef7","subtype":"command","commandType":"auto","position":2.5,"command":"val model = DecisionTree.trainClassifier(trainingData,numClasses,categoricalFeaturesInfo,impurity,maxDepth,maxBins)\nval model2 = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 1 with 3 nodes\nmodel2: org.apache.spark.mllib.tree.model.RandomForestModel = \nTreeEnsembleModel classifier with 50 trees\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:97: error: not found: value featureSubsetStrategy\n       val model2 = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,numTrees = 50, featureSubsetStrategy, impurity, maxDepth, maxBins)\n                                                                                                                  ^\n</div>","error":null,"workflows":[],"startTime":1.478642803059E12,"submitTime":1.478642803054E12,"finishTime":1.478642804062E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3144c0e2-2986-4c58-b95e-16a97d3be4cd"},{"version":"CommandV1","origId":1469190093917793,"guid":"af03e035-c56d-4e01-afa4-4305419ae0e8","subtype":"command","commandType":"auto","position":3.0,"command":"val labelAndPreds = testData.map{ point => val prediction = model.predict(point.features)\n  (point.label,prediction)}\nval labelAndPreds2 = testData.map{ point => val prediction = model2.predict(point.features)\n  (point.label,prediction)}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">labelAndPreds: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[527] at map at &lt;console&gt;:106\nlabelAndPreds2: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[528] at map at &lt;console&gt;:108\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:78: error: Double does not take parameters\n       val labelAndPreds = testData.map{ point =&gt; val prediction = model.predict(point.features)(point.label,prediction)}\n                                                                                                ^\n</div>","error":null,"workflows":[],"startTime":1.478642939922E12,"submitTime":1.478642939917E12,"finishTime":1.478642940213E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ccabf46e-d8fb-470e-af70-c53fd13bd7dd"},{"version":"CommandV1","origId":1469190093917795,"guid":"70d6a910-d6a5-4b77-ae08-9517f9bb5f5d","subtype":"command","commandType":"auto","position":4.0,"command":"val testErr =labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\nval testErr2 =labelAndPreds2.filter(r => r._1 != r._2).count.toDouble / testData.count()\nprintln(\"CART Test Error = \"+testErr)\nprintln(\"Learned classification tree model:\\n\"+model.toDebugString)\nprintln(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\nprintln(\"RF Test Error = \"+testErr2)\nprintln(\"RF details:\\n\"+model2.toDebugString)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">CART Test Error = 0.05714285714285714\nLearned classification tree model:\nDecisionTreeModel classifier of depth 1 with 3 nodes\n  If (feature 406 &lt;= 0.0)\n   Predict: 0.0\n  Else (feature 406 &gt; 0.0)\n   Predict: 1.0\n\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nRF Test Error = 0.0\nRF details:\nTreeEnsembleModel classifier with 50 trees\n\n  Tree 0:\n    If (feature 524 &lt;= 0.0)\n     If (feature 406 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 406 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 524 &gt; 0.0)\n     If (feature 243 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 243 &gt; 0.0)\n      Predict: 0.0\n  Tree 1:\n    If (feature 234 &lt;= 3.0)\n     If (feature 522 &lt;= 21.0)\n      Predict: 1.0\n     Else (feature 522 &gt; 21.0)\n      Predict: 0.0\n    Else (feature 234 &gt; 3.0)\n     If (feature 433 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 433 &gt; 0.0)\n      Predict: 1.0\n  Tree 2:\n    If (feature 517 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 517 &gt; 0.0)\n     Predict: 1.0\n  Tree 3:\n    If (feature 351 &lt;= 15.0)\n     Predict: 0.0\n    Else (feature 351 &gt; 15.0)\n     If (feature 482 &lt;= 83.0)\n      Predict: 1.0\n     Else (feature 482 &gt; 83.0)\n      Predict: 0.0\n  Tree 4:\n    If (feature 489 &lt;= 0.0)\n     If (feature 406 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 406 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 489 &gt; 0.0)\n     Predict: 1.0\n  Tree 5:\n    If (feature 518 &lt;= 16.0)\n     If (feature 490 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 490 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 518 &gt; 16.0)\n     Predict: 1.0\n  Tree 6:\n    If (feature 461 &lt;= 0.0)\n     If (feature 543 &lt;= 253.0)\n      Predict: 0.0\n     Else (feature 543 &gt; 253.0)\n      Predict: 1.0\n    Else (feature 461 &gt; 0.0)\n     Predict: 1.0\n  Tree 7:\n    If (feature 407 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 407 &gt; 0.0)\n     Predict: 1.0\n  Tree 8:\n    If (feature 511 &lt;= 0.0)\n     If (feature 517 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 517 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 511 &gt; 0.0)\n     Predict: 0.0\n  Tree 9:\n    If (feature 300 &lt;= 0.0)\n     If (feature 416 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 416 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 300 &gt; 0.0)\n     Predict: 0.0\n  Tree 10:\n    If (feature 378 &lt;= 36.0)\n     If (feature 545 &lt;= 150.0)\n      Predict: 0.0\n     Else (feature 545 &gt; 150.0)\n      Predict: 1.0\n    Else (feature 378 &gt; 36.0)\n     Predict: 1.0\n  Tree 11:\n    If (feature 455 &lt;= 23.0)\n     If (feature 521 &lt;= 99.0)\n      Predict: 1.0\n     Else (feature 521 &gt; 99.0)\n      Predict: 0.0\n    Else (feature 455 &gt; 23.0)\n     Predict: 0.0\n  Tree 12:\n    If (feature 232 &lt;= 0.0)\n     If (feature 407 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 407 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 232 &gt; 0.0)\n     Predict: 0.0\n  Tree 13:\n    If (feature 406 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 406 &gt; 0.0)\n     Predict: 1.0\n  Tree 14:\n    If (feature 569 &lt;= 252.0)\n     If (feature 346 &lt;= 4.0)\n      Predict: 1.0\n     Else (feature 346 &gt; 4.0)\n      Predict: 0.0\n    Else (feature 569 &gt; 252.0)\n     Predict: 0.0\n  Tree 15:\n    If (feature 406 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 406 &gt; 0.0)\n     Predict: 1.0\n  Tree 16:\n    If (feature 408 &lt;= 0.0)\n     If (feature 512 &lt;= 0.0)\n      If (feature 469 &lt;= 0.0)\n       Predict: 1.0\n      Else (feature 469 &gt; 0.0)\n       Predict: 0.0\n     Else (feature 512 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 408 &gt; 0.0)\n     Predict: 1.0\n  Tree 17:\n    If (feature 323 &lt;= 164.0)\n     If (feature 463 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 463 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 323 &gt; 164.0)\n     Predict: 1.0\n  Tree 18:\n    If (feature 434 &lt;= 0.0)\n     If (feature 627 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 627 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 434 &gt; 0.0)\n     Predict: 1.0\n  Tree 19:\n    If (feature 434 &lt;= 0.0)\n     If (feature 408 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 408 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 434 &gt; 0.0)\n     Predict: 1.0\n  Tree 20:\n    If (feature 378 &lt;= 0.0)\n     If (feature 519 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 519 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 378 &gt; 0.0)\n     Predict: 1.0\n  Tree 21:\n    If (feature 350 &lt;= 105.0)\n     If (feature 406 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 406 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 350 &gt; 105.0)\n     Predict: 1.0\n  Tree 22:\n    If (feature 517 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 517 &gt; 0.0)\n     Predict: 1.0\n  Tree 23:\n    If (feature 483 &lt;= 0.0)\n     Predict: 1.0\n    Else (feature 483 &gt; 0.0)\n     Predict: 0.0\n  Tree 24:\n    If (feature 407 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 407 &gt; 0.0)\n     Predict: 1.0\n  Tree 25:\n    If (feature 350 &lt;= 96.0)\n     If (feature 483 &lt;= 0.0)\n      If (feature 545 &lt;= 0.0)\n       Predict: 0.0\n      Else (feature 545 &gt; 0.0)\n       Predict: 1.0\n     Else (feature 483 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 350 &gt; 96.0)\n     Predict: 1.0\n  Tree 26:\n    If (feature 461 &lt;= 0.0)\n     If (feature 216 &lt;= 253.0)\n      Predict: 0.0\n     Else (feature 216 &gt; 253.0)\n      Predict: 1.0\n    Else (feature 461 &gt; 0.0)\n     Predict: 1.0\n  Tree 27:\n    If (feature 385 &lt;= 0.0)\n     If (feature 512 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 512 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 385 &gt; 0.0)\n     Predict: 0.0\n  Tree 28:\n    If (feature 413 &lt;= 0.0)\n     If (feature 316 &lt;= 98.0)\n      Predict: 1.0\n     Else (feature 316 &gt; 98.0)\n      Predict: 0.0\n    Else (feature 413 &gt; 0.0)\n     Predict: 0.0\n  Tree 29:\n    If (feature 441 &lt;= 0.0)\n     If (feature 483 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 483 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 441 &gt; 0.0)\n     Predict: 0.0\n  Tree 30:\n    If (feature 244 &lt;= 0.0)\n     If (feature 234 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 234 &gt; 0.0)\n      If (feature 271 &lt;= 0.0)\n       Predict: 1.0\n      Else (feature 271 &gt; 0.0)\n       Predict: 0.0\n    Else (feature 244 &gt; 0.0)\n     If (feature 546 &lt;= 253.0)\n      Predict: 0.0\n     Else (feature 546 &gt; 253.0)\n      Predict: 1.0\n  Tree 31:\n    If (feature 435 &lt;= 0.0)\n     If (feature 462 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 462 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 435 &gt; 0.0)\n     Predict: 1.0\n  Tree 32:\n    If (feature 461 &lt;= 0.0)\n     If (feature 351 &lt;= 15.0)\n      Predict: 0.0\n     Else (feature 351 &gt; 15.0)\n      Predict: 1.0\n    Else (feature 461 &gt; 0.0)\n     Predict: 1.0\n  Tree 33:\n    If (feature 407 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 407 &gt; 0.0)\n     Predict: 1.0\n  Tree 34:\n    If (feature 385 &lt;= 0.0)\n     If (feature 433 &lt;= 0.0)\n      If (feature 238 &lt;= 77.0)\n       Predict: 0.0\n      Else (feature 238 &gt; 77.0)\n       If (feature 599 &lt;= 84.0)\n        Predict: 1.0\n       Else (feature 599 &gt; 84.0)\n        Predict: 0.0\n     Else (feature 433 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 385 &gt; 0.0)\n     Predict: 0.0\n  Tree 35:\n    If (feature 428 &lt;= 0.0)\n     If (feature 316 &lt;= 98.0)\n      If (feature 684 &lt;= 87.0)\n       Predict: 1.0\n      Else (feature 684 &gt; 87.0)\n       If (feature 660 &lt;= 0.0)\n        Predict: 1.0\n       Else (feature 660 &gt; 0.0)\n        Predict: 0.0\n     Else (feature 316 &gt; 98.0)\n      Predict: 0.0\n    Else (feature 428 &gt; 0.0)\n     Predict: 0.0\n  Tree 36:\n    If (feature 300 &lt;= 3.0)\n     If (feature 386 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 386 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 300 &gt; 3.0)\n     Predict: 0.0\n  Tree 37:\n    If (feature 596 &lt;= 0.0)\n     If (feature 579 &lt;= 70.0)\n      Predict: 1.0\n     Else (feature 579 &gt; 70.0)\n      Predict: 0.0\n    Else (feature 596 &gt; 0.0)\n     If (feature 490 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 490 &gt; 0.0)\n      Predict: 1.0\n  Tree 38:\n    If (feature 512 &lt;= 0.0)\n     If (feature 545 &lt;= 0.0)\n      Predict: 0.0\n     Else (feature 545 &gt; 0.0)\n      Predict: 1.0\n    Else (feature 512 &gt; 0.0)\n     Predict: 0.0\n  Tree 39:\n    If (feature 407 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 407 &gt; 0.0)\n     Predict: 1.0\n  Tree 40:\n    If (feature 428 &lt;= 0.0)\n     If (feature 567 &lt;= 167.0)\n      Predict: 1.0\n     Else (feature 567 &gt; 167.0)\n      Predict: 0.0\n    Else (feature 428 &gt; 0.0)\n     Predict: 0.0\n  Tree 41:\n    If (feature 401 &lt;= 0.0)\n     If (feature 358 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 358 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 401 &gt; 0.0)\n     Predict: 0.0\n  Tree 42:\n    If (feature 350 &lt;= 105.0)\n     If (feature 324 &lt;= 38.0)\n      Predict: 0.0\n     Else (feature 324 &gt; 38.0)\n      If (feature 568 &lt;= 0.0)\n       Predict: 1.0\n      Else (feature 568 &gt; 0.0)\n       Predict: 0.0\n    Else (feature 350 &gt; 105.0)\n     Predict: 1.0\n  Tree 43:\n    If (feature 385 &lt;= 0.0)\n     If (feature 400 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 400 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 385 &gt; 0.0)\n     Predict: 0.0\n  Tree 44:\n    If (feature 442 &lt;= 0.0)\n     If (feature 318 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 318 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 442 &gt; 0.0)\n     Predict: 0.0\n  Tree 45:\n    If (feature 261 &lt;= 0.0)\n     If (feature 483 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 483 &gt; 0.0)\n      Predict: 0.0\n    Else (feature 261 &gt; 0.0)\n     Predict: 0.0\n  Tree 46:\n    If (feature 517 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 517 &gt; 0.0)\n     Predict: 1.0\n  Tree 47:\n    If (feature 350 &lt;= 105.0)\n     If (feature 604 &lt;= 253.0)\n      If (feature 568 &lt;= 0.0)\n       Predict: 1.0\n      Else (feature 568 &gt; 0.0)\n       Predict: 0.0\n     Else (feature 604 &gt; 253.0)\n      Predict: 1.0\n    Else (feature 350 &gt; 105.0)\n     Predict: 1.0\n  Tree 48:\n    If (feature 489 &lt;= 0.0)\n     Predict: 0.0\n    Else (feature 489 &gt; 0.0)\n     Predict: 1.0\n  Tree 49:\n    If (feature 545 &lt;= 91.0)\n     If (feature 161 &lt;= 162.0)\n      Predict: 0.0\n     Else (feature 161 &gt; 162.0)\n      Predict: 1.0\n    Else (feature 545 &gt; 91.0)\n     If (feature 385 &lt;= 0.0)\n      Predict: 1.0\n     Else (feature 385 &gt; 0.0)\n      Predict: 0.0\n\ntestErr: Double = 0.05714285714285714\ntestErr2: Double = 0.0\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:127: error: not found: value labelAndPreds2\n              val testErr2 =labelAndPreds2.filter(r =&gt; r._1 != r._2).count.toDouble / testData.count()\n                            ^\n</div>","error":null,"workflows":[],"startTime":1.478642986312E12,"submitTime":1.478642986307E12,"finishTime":1.478642986624E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8f1a215e-b3d8-40f3-9d54-b1bbe153ae2c"}],"dashboards":[],"guid":"92a7a6f8-f810-482c-ad4f-06e944a11121","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
